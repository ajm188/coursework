\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}

\title{EECS 440 HW5}
\author{Andrew Mason}

\begin{document}
\maketitle

\begin{enumerate}
  \item
    % 1
    It may be beneficial to overfit if you know that real data will have about
    the same distribution as the training data. That is, the closer the
    training data is to ``real world'' data, the more beneficial overfitting
    is. The reason to avoid overfitting is to prevent the model from performing
    poorly later due to high variance between the training data and real
    samples. However, if there is no (or little) variance, then an overfit
    model will perform extremely well both in evaluation and on real samples.\\
  \item
    % 2
    Sure; this approach is strikingly similar to $n$-fold cross validation. The
    test sets are drawn independently of each other, which is the most important
    aspect of cross validation. The only issue that this approach might cause is
    that the train and test sets are the same size, where you would probably get
    better results if you trained on larger sets.\\
  \item
    % 3
    \begin{proof} The Precision-Recall curve of A dominates the
      Precision-Recall curve of B.\\
      Let $P$ be the number of actual positives.\\
      Let $N$ be the number of actual negatives.\\

      Assume that the PR curve of A does not dominate the PR curve of B. Then
      there is a point such that the recall of A and B are equal, but
      $Precision(B)>Precision(A)$.\\

      Since $Recall(A)=Recall(B)=TPR(A)=TPR(B)$, we have
      $\frac{TP_A}{P}=\frac{TP_B}{P}$, so $TP_A=TP_B$, which we can now just
      call $TP$.

      Since the ROC curve of A dominates B`s, we also have that $FPR(B)\geq
      FPR(A)$, so $\frac{FP_B}{N}\geq\frac{FP_A}{N}$, so $FP_B\geq FP_A$.\\

      Now, if we look at the precision of A and B, we have
      \begin{equation}
        \begin{split}
          Precision(A)&=\frac{TP}{FP_A+TP}\\
          Precision(B)&=\frac{TP}{FP_B+TP}\\
        \end{split}
      \end{equation}

      From this, we have that $Precision(B)\leq Precision(A)$, which
      contradicts with the original assumption that $Precision(B)>Precision(A)$
      , so the Precision-Recall curve of A must dominate the Precision-Recall
      curve of B.\\
    \end{proof}
  \item
    % 4
    \begin{enumerate}
      \item
        \begin{proof} The ROC graph must be monotonically increasing.\\
        By induction on the confidence threshold.\\
        Let $P$ be the number of positive examples and $N$ be the number of
        negative examples.\\
        If the confidence is 0, then both the true positive rate and the
        false postive rate are 0. So the ROC curve starts at (0,0).\\

        If the confidence is increased to mark one more example as positive:\\

        Case 1: The example is actually positive.\\
        True positive rate increases by $\frac{1}{P}$. This moves the graph
        upward (vertically increasing).\\

        Case 2: The example is actually negative.\\
        False positive rate increases by $\frac{1}{N}$. This moves the graph to
        the right (horizontally increasing).\\

        In either case, the graph increases. Thus, it does not decrease.\\
        So by induction, the entire ROC curve is non-decreasing.\\
        \end{proof}
      \item
        For a majority class classifier, all examples are labelled positive (or
        negative) with a confidence of 1. As a result, there are only two
        points on the ROC curve. One point is for the confidence threshold of
        1, in which none of the examples are labelled positive, so both the TPR
        and FPR rates are 0. The other point is for the confidence threshold of
        0, in which all of the examples are labelled positive, so the TPR is
        the number of positive examples divided by the number of total
        examples, and the false positive rate is the number of negative
        examples divided by the total number of examples. So the ROC curve is
        the straight line connecting (0, 0) to this point.\\
      \item
        For this classifier, each example will be be assigned a confidence of 0
        or 1, each with equal probability. So, by a similar argument to part b,
        we have only two points on the ROC curve. One is at confidence 1, where
        none of the examples are labelled positive (and TPR=FPR=0), and the
        other is at $0<\text{confidence}<1$, where all the examples that were
        randomly assigned the positive label (confidence 1) are positive. The
        TPR and FPR don`t really matter here (both are nonzero), since the ROC
        curve is still the straight line connecting these two points.\\
    \end{enumerate}
  \item
    % 5
\end{enumerate}
\end{document}
