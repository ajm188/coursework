\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}

\title{EECS 440 HW4}
\author{Andrew Mason}

\begin{document}
\maketitle

\begin{enumerate}
  \item
    % 1
    \begin{enumerate}
      \item
        Memorization should not be considered learning, because part of
        learning is being able to take concepts you have "learned" and apply
        them to new areas. When you memorize something, you "learn" that one
        thing and only that one thing. For example, memorizing the
        multiplication tables allows you to recall that $2\times 2=4$, but
        teaches you nothing about sales taxes, tips, or gratuities.
      \item
        Tabula rasa learning is not possible. Even if there were an algorithm
        which could learn from any training set, exploring every possible
        hypothesis is unfeasible, since there could be infinitely many valid
        hypotheses, so the hypothesis space must be restricted in some way. The
        way that the space is restricted is determined by the designer of the
        learning algorithm, and this bias is inherently built in to the system,
        so it is actually not tabula rasa.
      \item
        Choosing a good example representation is important for learning
        because a good example representation makes it easier (or easiest) for
        the learning agent to understand and interpret the training examples.
        Using an overly-complicated or nonsensical representation slows down
        the learning process by making things unnecessarily difficult for the
        learner. Choosing a bad example representation would be like teaching
        a child what lions look like by using strings of DNA encoding as the
        examples. Showing them pictures (or taking them to a zoo), would be far
        easier and more efficient.
    \end{enumerate}
  \item
    % 2
    I do not think it would be possible to have a "best" learning algorithm. If
    a single algorithm was superior for all learning tasks, then we could just
    blindly use this algorithm everywhere and ignore any prior knowledge of a
    particular learning task, since this algorithm is better anyways. In
    effect, tabular rasa learning would be possible if such an algorithm
    existed, since this algorthim would be doing tabular rasa learning.
  \item
    % 3
    \begin{proof} For a binary classification task, the information gain for
    any binary split variable, $X$, is always nonnegative.\\

    Let $Y$ be the random variable which takes on the value of the class
    label. Then, $IG(X)=H(Y)-H(Y|X)$.\\

    Define mutual information $I(X;Y)$ to be
    $$\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$$\\

    Then,
    \begin{equation}
      \begin{split}
      I(X;Y)&=\sum_{x,y}p(x,y)\log\frac{p(y|x)}{p(x)}\\
      &=H(Y)-H(Y|X)\\
      \end{split}
    \end{equation}

    Applying Jensen's inequality to any term of the sum, we have
    \begin{equation}
      \begin{split}
      -p(x,y)\log\frac{p(x,y)}{p(x)p(y)}&=-p(x,y)\log\frac{p(x)p(y)}{p(x,y)}\\
      &\leq\log p(x,y)\frac{p(x)p(y)}{p(x,y)}\\
      &\leq\log p(x)p(y)\\
      \end{split}
    \end{equation}

    Now,
    \begin{equation}
      \begin{split}
      -I(X;Y)&=\sum_{x,y}-p(x,y)\log\frac{p(x)p(y)}{p(x,y)}\\
      &\leq\log\sum_{x,y}p(x,y)\\
      &\leq\log 1\\
      &\leq 0\\
      \end{split}
    \end{equation}

    So, $I(X;Y)$ is nonnegative, as is information gain.\\
    \end{proof}
  \item
    % 4
    \begin{proof} For a continuous attribute $X$, max $IG(X)$ occurs between
      points with different labels.\\

      Considering the setup given in the assignment (candidate split point $S$
      in the middle of $N$ examples with the same labels, to the right of $n$ such
      examples; to the left there are $L_0$ examples with negative label and $L_1$
      examples with positive label and likewise to the right ($R_0$, $R_1$)), we can
      define $IG(S)$ as a function of $n$.\\

      $IG(S,n)=H(Y)-H(Y|S\leq n)\ n\in[0,N]$\\

      Since $H(Y)$ does not depend on $n$, maximizing $IG(S,n)$ is
      equivalent to maximizing $-H(Y|S\leq n)$ or minimizing $H(Y|S\leq n)$.\\

      Now, $H(X)$ is a concave function, so to find a minimum over [0,N], we only
      need to consider the extreme points of the interval. This means that
      $-H(Y|S\leq n)$ (and, by extension, $IG(S,n)$), is maximized either when
      $n=0$ or $n=N$.\\
    \end{proof}
\end{enumerate}
\end{document}
